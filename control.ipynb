{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de los módulos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importan los módulos necesarios para el entrenamiento del modelo de control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl.nn as dglnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import networkx as nx\n",
    "import random\n",
    "from statistics import mean\n",
    "import time\n",
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de generación de grafo aleatorio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función genera un grafo completo ponderado con pesos aleatorios usado en lso ejemplos de la evaluación del coste medio de los caminos generados por el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_tsp_graph(n_cities, min_distance=1, max_distance=100):\n",
    "    G = nx.Graph()\n",
    "    for i in range(n_cities):\n",
    "        G.add_node(i)\n",
    "\n",
    "    for i in range(n_cities):\n",
    "        for j in range(i+1, n_cities):\n",
    "            distance = random.randint(min_distance, max_distance)\n",
    "            G.add_edge(i, j, weight=distance)\n",
    "\n",
    "    return G\n",
    "\n",
    "def graph_to_adjacency_matrix(G):\n",
    "    return nx.adjacency_matrix(G).todense().tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen las constantes como el número de clases de la predicción y la longitud de las secuencias.\n",
    "Se lee el dataset previamente generado en `dataset_generation_processing.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 14\n",
    "NUM_CLASSES_OUT = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_full_adj_matrix_15_nodes_stock.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función usada para obtener los índices no redundantes de una matriz de adyacencia de tamaño NxN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función lista los índices de los coeficientes situados bajo la diagonal principal de una matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_diagonal_indices(n):\n",
    "    # List to store the indices of elements below the diagonal\n",
    "    indices = []\n",
    "\n",
    "    # Loop through each row\n",
    "    for i in range(1, n):  # Start from 1 because the first row does not have any elements below the diagonal\n",
    "        # Loop through each column up to (but not including) the diagonal\n",
    "        for j in range(i):\n",
    "            # Compute the index in the flattened matrix and add to the list\n",
    "            index = i * n + j\n",
    "            indices.append(index)\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de la matriz de adyacencia a partir del dataset\n",
    "\n",
    "Se leen los coeficientes y se convierten en un tensor que contiene 14000 ejemplos de matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14000, 15, 15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = ['col_' + str(i) for i in range(1, NUM_CLASSES_OUT**2 + 1)]\n",
    "selected_data = df[selected_columns]\n",
    "\n",
    "numpy_array = selected_data.values\n",
    "tensor = torch.tensor(numpy_array).view(df.shape[0], NUM_CLASSES_OUT, NUM_CLASSES_OUT)\n",
    "adjacency_matrices = tensor\n",
    "adjacency_matrices.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza la función descrita con anterioridad para obtener los coeficientes bajo la diagonal principal de cada matriz de adyacencia del tesnor.  \n",
    "Cada matriz de 15x15 genera 105 predictores correspondientes a los coeficientes no repetidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_indices = below_diagonal_indices(NUM_CLASSES_OUT)\n",
    "flat_matrices = torch.zeros((df.shape[0], (NUM_CLASSES_OUT**2 - NUM_CLASSES_OUT) // 2))\n",
    "for i in range(df.shape[0]):\n",
    "    for index, j in enumerate(relevant_indices):\n",
    "        flat_matrices[i][index] = df[f'col_{j+1}'][i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de las matrices de adyacencia\n",
    "\n",
    "Se normalizan los coeficientes de las matrices entre -0.5 y 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9010, 0.5842, 0.9208,  ..., 0.7129, 0.3960, 0.9010],\n",
       "        [0.9010, 0.5842, 0.9208,  ..., 0.7129, 0.3960, 0.9010],\n",
       "        [0.9010, 0.5842, 0.9208,  ..., 0.7129, 0.3960, 0.9010],\n",
       "        ...,\n",
       "        [0.7723, 0.1683, 0.4851,  ..., 0.7822, 0.8812, 0.9307],\n",
       "        [0.7723, 0.1683, 0.4851,  ..., 0.7822, 0.8812, 0.9307],\n",
       "        [0.7723, 0.1683, 0.4851,  ..., 0.7822, 0.8812, 0.9307]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_matrices = flat_matrices / 101\n",
    "flat_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14000, 105])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_indices = flat_matrices\n",
    "graph_indices = graph_indices - 0.5\n",
    "graph_indices.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de las secuencias de nodos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen los caminos parciales que componen la parte secuencial de los problemas de clasificación, se leen del dataframe y se convierten en enteros.  \n",
    "Se forma un tensor de 14.000 ejemplos con las secuencias obtenidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, -1, -1,  ..., -1, -1, -1],\n",
       "        [ 0, 12, -1,  ..., -1, -1, -1],\n",
       "        [ 0, 12,  4,  ..., -1, -1, -1],\n",
       "        ...,\n",
       "        [ 0,  2, 10,  ...,  7, -1, -1],\n",
       "        [ 0,  2, 10,  ...,  7, 14, -1],\n",
       "        [ 0,  2, 10,  ...,  7, 14,  8]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_tensor = torch.zeros((df.shape[0], NUM_CLASSES_OUT - 1))\n",
    "for i in range(df.shape[0]):\n",
    "    for j in range(SEQUENCE_LENGTH):\n",
    "        sequence_tensor[i][j] = df[f'seq_{j+1}'][i]\n",
    "\n",
    "sequence_tensor = sequence_tensor.long()\n",
    "sequence_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación de las secuencias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza una combinación de advanced indexing y tensores booleanos (masks) para codificar cada elemento (nodo) de la secuencia con su fila correspondiente en la matriz de adyacencia del ejemplo (grafo) en el que aparece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices = torch.arange(len(sequence_tensor)).view(-1, 1).expand_as(sequence_tensor)\n",
    "mask = (sequence_tensor == -1)\n",
    "\n",
    "sequence_tensor = sequence_tensor.masked_fill(mask, 0)\n",
    "\n",
    "sequence_encodings = adjacency_matrices[batch_indices, sequence_tensor]\n",
    "sequence_encodings = sequence_encodings.masked_fill(mask.unsqueeze(-1), 0)\n",
    "\n",
    "sequence_tensor = sequence_tensor.masked_fill(mask, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_encodings = sequence_encodings / 101\n",
    "sequence_encodings[sequence_encodings != 0] -= 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de los objetivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un tensor leyendo los objetivos almacenados en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_tensor = torch.tensor(df['next_node'])\n",
    "targets_tensor.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicion del modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de control consiste en: \n",
    "1. Una capa Linear que procesa los coeficientes de las matrices de adyacencia.\n",
    "2. Una capa Linear que procesa las secuencias de nodos que componen los caminos parciales.\n",
    "3. Una capa Linear que toma las salidas de las capas anteriores como entrada y devuelve la clasificación final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlModel(nn.Module):\n",
    "    def __init__(self, sequence_embed_size):\n",
    "        super(ControlModel, self).__init__()\n",
    "\n",
    "        # Adjacency matrix processing layer\n",
    "        self.matrix_proc = nn.Linear((NUM_CLASSES_OUT**2 - NUM_CLASSES_OUT) // 2, NUM_CLASSES_OUT)\n",
    "        \n",
    "        # Node sequence processing layer\n",
    "        self.sequence_proc = nn.Linear((NUM_CLASSES_OUT- 1) * NUM_CLASSES_OUT, sequence_embed_size)\n",
    "\n",
    "        # Fully Connected Output Layer\n",
    "        self.fc_out = nn.Linear(NUM_CLASSES_OUT + sequence_embed_size, NUM_CLASSES_OUT)\n",
    "        \n",
    "        # Activation Function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, flat_matrix, sequence_encoding):\n",
    "        # Process adjacency matrix through Adjacency matrix processing layer\n",
    "        matrix_out = self.relu(self.matrix_proc(flat_matrix))\n",
    "\n",
    "        # Flatten sequence input\n",
    "        sequence_encoding = sequence_encoding.reshape(sequence_encoding.shape[0], -1)\n",
    "\n",
    "        # Process sequence through a Fully Connected Layer\n",
    "        sequence_ouput = self.relu(self.sequence_proc(sequence_encoding))\n",
    "\n",
    "        # Concatenate outputs from matrix_proc and sequence_proc\n",
    "        concat_output = torch.cat((matrix_out, sequence_ouput), dim=1)\n",
    "        \n",
    "        # Feed concatenated output through Fully Connected Layer\n",
    "        out = self.relu(self.fc_out(concat_output))\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & Dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una clase hija de la clase Dataset de Pytorch para almacenar los datos.  \n",
    "Esto permite mayor flexibilidad para situar la codificación de los datos en distintas partes del código y ha sido extremadamente útil durante el desarrollo de este trabajo.  \n",
    "Actualmente la implementación del experimento podría hacerse con la clase Dataset directamente pero se deja implementada esta clase por si fuese útil para la continuación futura de la investigación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGraphSequenceDataset(Dataset):\n",
    "    def __init__(self, sequence_tensor, adj_matrices, targets_tensor):\n",
    "        self.sequence_tensor = sequence_tensor\n",
    "        self.adj_matrices = adj_matrices\n",
    "        self.targets_tensor = targets_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequence_tensor[idx]\n",
    "        adj_matrix = self.adj_matrices[idx]\n",
    "        target = self.targets_tensor[idx]\n",
    "        return sequence, adj_matrix, target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma similar a la clase anterior, esta función collate personalizada es extremadamente útil a la hora de hacer pruebas con las diversas posibilidades para transformar los grupos de ejemplos en batches.  \n",
    "Queda en el código por si viese más utilidad en la investigación futura, a pesar de que actualmente los scripts funcionarían con la función collate por defecto de pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    sequences, graphs, targets = zip(*batch)\n",
    "    # Convert sequences to a single tensor\n",
    "    sequences = torch.stack(sequences)\n",
    "    # Convert targets to a single tensor\n",
    "    targets = torch.stack(targets)\n",
    "    # Batch graphs into a single graph\n",
    "    batched_graph = torch.stack(graphs) # dgl.batch(graphs)\n",
    "    return sequences, batched_graph, targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se dividen las codificaciones de las secuencias, las matrices de adyacencia y los objetivos entre conjuntos de entrenamiento y validación.\n",
    "Se crean los datasets de entrenamiento y test a partir de los tensores resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "train_sequences, test_sequences, train_adj_matrices, test_adj_matrices, train_targets, test_targets = train_test_split(\n",
    "    sequence_encodings, graph_indices, targets_tensor, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Custom Dataset for train and test\n",
    "train_dataset = CustomGraphSequenceDataset(train_sequences, train_adj_matrices, train_targets)\n",
    "test_dataset = CustomGraphSequenceDataset(test_sequences, test_adj_matrices, test_targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del coste medio de los caminos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función recibe un modelo como parámetro y se ejecuta durante la fase de evaluación para calcular el coste medio del camino generado por este modelo.\n",
    "\n",
    "Esta función genera un batch de grafos completos ponderados con pesos aleatorios.  \n",
    "Una vez generados, se obtienen las matrices de adyacencia correspondientes a los grafos y se procesan sus coeficientes tal como se hace en el apartado anterior (Obtención de la matriz de adyacencia a partir del dataset)..\n",
    "\n",
    "Se generan tensores correspondientes a secuencias de nodos vacías (todas tienen 0 como nodo inicial como en los ejemplos del dataset).\n",
    "Se codifican dichas secuencias y se generan los elementos siguientes a partir de predicciones del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_evaluation(model, batch_size):\n",
    "    matrices_tensor_batch = torch.zeros((batch_size, (NUM_CLASSES_OUT**2 - NUM_CLASSES_OUT) // 2))\n",
    "    adjacency_matrices = np.ndarray((batch_size, NUM_CLASSES_OUT, NUM_CLASSES_OUT))\n",
    "\n",
    "    # Generate one batch of adjacency matrices \n",
    "    for tensor_row in range(batch_size):\n",
    "        graph = generate_random_tsp_graph(NUM_CLASSES_OUT)\n",
    "        adjacency_matrix = graph_to_adjacency_matrix(graph)\n",
    "        adjacency_matrices[tensor_row] = torch.tensor(adjacency_matrix).view((NUM_CLASSES_OUT, NUM_CLASSES_OUT))\n",
    "        \n",
    "        partial_matrix = list()\n",
    "        for relevant_index in relevant_indices:\n",
    "            flattened_adj_matrix = [item for sublist in adjacency_matrix for item in sublist]\n",
    "            partial_matrix.append(flattened_adj_matrix[relevant_index])\n",
    "        \n",
    "        matrices_tensor_batch[tensor_row] = torch.tensor(partial_matrix)\n",
    "\n",
    "    # Normalize\n",
    "    matrices_tensor_batch = matrices_tensor_batch / 101\n",
    "\n",
    "    # Generate an empty sequence from a random starting point\n",
    "    sequences_tensor_batch = torch.zeros((batch_size, NUM_CLASSES_OUT - 1)).long()\n",
    "    sequences_tensor_batch[:, 0] = torch.tensor(np.random.randint(0, 15, size=(batch_size)))\n",
    "\n",
    "    # Store the decision cost in each step of the way\n",
    "    costs = torch.zeros((NUM_CLASSES_OUT - 2, batch_size))\n",
    "    for i in range(NUM_CLASSES_OUT - 2):\n",
    "        # Encode the sequence for the model\n",
    "        batch_indices = torch.arange(len(sequences_tensor_batch)).view(-1, 1).expand_as(sequences_tensor_batch)\n",
    "        mask = (sequences_tensor_batch == -1)\n",
    "\n",
    "        sequences_tensor_batch = sequences_tensor_batch.masked_fill(mask, 0)\n",
    "\n",
    "        sequence_encodings = torch.tensor(adjacency_matrices[batch_indices, sequences_tensor_batch])\n",
    "        sequence_encodings = sequence_encodings.masked_fill(mask.unsqueeze(-1), 0)\n",
    "\n",
    "        sequences_tensor_batch = sequences_tensor_batch.masked_fill(mask, -1)\n",
    "\n",
    "        sequence_encodings = sequence_encodings / 101\n",
    "        sequence_encodings[sequence_encodings != 0] -= 0.5\n",
    "        \n",
    "        sequence_encodings = sequence_encodings.to(torch.float32)\n",
    "\n",
    "        outputs = model(matrices_tensor_batch, sequence_encodings)\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        previous_labels = sequences_tensor_batch[:, i]\n",
    "        costs_this_timestep = list()\n",
    "        for batch_index in range(batch_size):\n",
    "            matrix = adjacency_matrices[batch_index]\n",
    "            # print(len(matrix))\n",
    "            # print(predicted_labels[batch_index])\n",
    "            # print(previous_labels[batch_index])\n",
    "            costs_this_timestep.append(matrix[predicted_labels[batch_index]][previous_labels[batch_index]])\n",
    "\n",
    "        costs_this_timestep = torch.tensor(costs_this_timestep)\n",
    "        costs[i] = costs_this_timestep\n",
    "\n",
    "    # Calculate total cost of each path\n",
    "    costs = torch.sum(costs, dim=0)\n",
    "\n",
    "    # Return the mean value from al lthe path costs\n",
    "    return torch.mean(costs).item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciamiento y loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente funcion instancia un modelo con los hiperparámetros pasados como argumentos, y lo entrena durante 100 épocas.\n",
    "Para ello utiliza el algoritmo de optimización Adam y la función de pérdida CrossEntropyLoss.  \n",
    "Se crea un archivo de registro con los resultados del modelo en el directorio results, que contiene en el nombre los hiperparámetros usados.  \n",
    "Durante la fase de evaluación, además de comprobar la precisión del modelo sobre el conjunto de test, se evalúa el coste medio en los caminos generados por este sobre un batch de ejemplos generados aleatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, test_loader, hyperparams, identifier, batch_size):\n",
    "    learning_rate = hyperparams['learning_rate']\n",
    "    hidden_size = hyperparams['hidden_size']\n",
    "\n",
    "    # Initialize new model, optimizer and loss function\n",
    "    model = ControlModel(sequence_embed_size=hidden_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=1e-5)\n",
    "\n",
    "    # Number of epochs\n",
    "    num_epochs = 100\n",
    "    total_path_costs_mean = list()\n",
    "\n",
    "\n",
    "    csv_file = open(f\"./results/control/{batch_size}/{identifier}.csv\", mode=\"a\", newline=\"\")\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"Epoch\", \"Train_Accuracy\", \"Test_Accuracy\", \"Mean_Path_Cost\", \"Elapsed_Time\"])\n",
    "\n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        batch_num = len(train_loader)\n",
    "        for batch_idx, (sequences, batched_graph, targets) in enumerate(train_loader):\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batched_graph, sequences)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the training accuracy\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            total_samples += targets.size(0)\n",
    "            correct_predictions += (predicted_labels == targets).sum().item()\n",
    "        \n",
    "        # Compute the training accuracy for the epoch\n",
    "        accuracy = correct_predictions / total_samples\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "        mean_this_epoch = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sequences, batched_graph, targets in test_loader:\n",
    "                # Forward pass\n",
    "                outputs = model(batched_graph, sequences)\n",
    "\n",
    "                # Update the testing accuracy\n",
    "                _, predicted_labels = torch.max(outputs, 1)\n",
    "                total_samples += targets.size(0)\n",
    "                correct_predictions += (predicted_labels == targets).sum().item()\n",
    "\n",
    "            # Path evaluation\n",
    "            mean_this_epoch = path_evaluation(model, batch_size)\n",
    "            total_path_costs_mean.append(mean_this_epoch)\n",
    "\n",
    "        # Compute the testing accuracy for the epoch\n",
    "        test_accuracy = correct_predictions / total_samples\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        csv_writer.writerow([\n",
    "            epoch + 1,\n",
    "            round(accuracy * 100, 4),\n",
    "            round(test_accuracy * 100, 4),\n",
    "            round(mean_this_epoch, 4),\n",
    "            round(elapsed_time, 4)\n",
    "        ])\n",
    "        csv_file.flush()\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "        print(f'Train Accuracy: {accuracy * 100:.2f}%, Testing Accuracy: {test_accuracy * 100:.2f}%')\n",
    "        print(f'Path cost mean this epoch: {mean_this_epoch:.2f}, Total path cost mean : {mean(total_path_costs_mean):.2f}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid de hiperparámetros\n",
    "En esta última función se especifican los distintos valores que tendrán los hiperparámetros del grid, y se entrena al modelo previamente definido utilizando todas las combinaciones posibles de hiperparámetros para ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "batch_sizes =  (16, 32, 64)\n",
    "\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [0.0001, 0.0005, 0.001],\n",
    "    'hidden_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print('current batch size is ' + str(batch_size))\n",
    "\n",
    "    all_combs = product(*(hyperparameter_grid[hp] for hp in hyperparameter_grid))\n",
    "    for comb in all_combs:\n",
    "        hyperparams = dict(zip(hyperparameter_grid.keys(), comb))\n",
    "\n",
    "        identifier = '_'.join([f\"{k}={v}\" for k, v in hyperparams.items()])\n",
    "        \n",
    "        print(f\"Training with {identifier}\")\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)\n",
    "        \n",
    "        train_model(train_loader, test_loader, hyperparams, identifier, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
